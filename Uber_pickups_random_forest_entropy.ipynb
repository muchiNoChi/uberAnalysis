{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZcGWadzEhW9Q"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import operator\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L___pHoJnYYQ"
   },
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kuk0Y_AunptR"
   },
   "source": [
    "For this problem '*Uber Pickups*' dataset was chosen. It represents the data for different New York Uber pickups.\n",
    "Our goal is, by analysing this dataset, to predict the amount of pickups for each borough presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "colab_type": "code",
    "id": "PhboEoPpjdjn",
    "outputId": "07574509-b85b-4224-edde-b3cba8434b8e"
   },
   "outputs": [],
   "source": [
    "X_data = pd.read_csv('data/trainX.csv')\n",
    "Y_data = pd.read_csv('data/trainY.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "NAVLq5w_kQSM",
    "outputId": "be13d47f-477c-4b6d-cdd2-8913139acf81"
   },
   "outputs": [],
   "source": [
    "X_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ta4apuXIkUXd"
   },
   "source": [
    "For a solution using scikit-learn we considered 3 types of classifiers: *RandomForest*, *DecisionTree* and *LogisticRegression*.\n",
    "\n",
    "(See solution using scikit-learn for details)\n",
    "\n",
    "The following results (in points) were achieved by fitting (after applying Grid Search with different parameters on those classifiers (specific solvers were chosen)):\n",
    "\n",
    "*   RandomForest (entropy) - **1246401.3858999142**\n",
    "*   DecisionTree (gini) - **1245676.0787156357**\n",
    "*   LogisticRegression (newton-cg) - **1193406.0331829898**\n",
    "\n",
    "According to these results, the best algorithm to apply on our dataset (between those 3 been tried) is ***RandomForest***.\n",
    "It will be implemented in this work to be then applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yFWEgXCxnSgb"
   },
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O6Lt0dlAgPYH"
   },
   "source": [
    "First, we don't need a pickup **id**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I59QdW_bgU4_"
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X_data, copy=True)\n",
    "del X['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G9XR28pNgin6"
   },
   "source": [
    "Second, we'll transform **pickup_dt** feature into *numeric* type (as done in a solution using scikit-learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VlrszzyLgxpX"
   },
   "outputs": [],
   "source": [
    "X['pickup_dt'] = X['pickup_dt'].apply(\n",
    "    lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").time()\n",
    ").apply(\n",
    "    lambda x: x.second+x.minute*60+x.hour*3600\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cvil0uG-hHQQ"
   },
   "source": [
    "Third, let's transform **holiday** char typed feature into *binary* typed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tdIXMfckhVk0"
   },
   "outputs": [],
   "source": [
    "X['hday'] = X['hday'].apply(lambda x: 0 if x == 'N' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "4Yi_giA2hZq8",
    "outputId": "d4163322-64ab-4b56-95ad-464edab853b1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WtAnYg60ibYx"
   },
   "source": [
    "Now let's normalize the data. We will use numpy's *linalg.norm()* function. It won't be implemented here as it's big enough, and you always can take a look at the source code via [this link](https://github.com/numpy/numpy/blob/v1.15.1/numpy/linalg/linalg.py#L2203-L2440)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g8cCRIJlizzC"
   },
   "outputs": [],
   "source": [
    "def normalize(v):\n",
    "  norm = np.linalg.norm(v)\n",
    "  if norm == 0:\n",
    "    return v\n",
    "  return v / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PechGMftjdIl"
   },
   "outputs": [],
   "source": [
    "for column_name in ['pickup_dt', 'spd', 'vsb', 'temp', 'dewp', \n",
    "                    'slp', 'pcp01', 'pcp06', 'pcp24', 'sd']:\n",
    "  X[column_name] = normalize(X[column_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "gyM09AMtjqRC",
    "outputId": "32e4638d-bba3-4c34-e7f3-36dbadb25827",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data-binning for categorizing continous values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for column_name in ['pickup_dt', 'spd', 'vsb', 'temp', 'dewp', \n",
    "                    'slp', 'pcp01', 'pcp06', 'pcp24', 'sd']:\n",
    "    bins = []\n",
    "#     we will create 20 bins to try\n",
    "    step = (X[column_name].max() - X[column_name].min()) / 20\n",
    "    for i in range(21):\n",
    "        bins.append(X[column_name].min() + (step * i))\n",
    "    binned_column_name = '{}_binned'.format(column_name)\n",
    "    X[binned_column_name] = pd.cut(X[column_name], bins=bins)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's aggregate amount of values for bins and group them by size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_name in ['pickup_dt', 'spd', 'vsb', 'temp', 'dewp', \n",
    "                    'slp', 'pcp01', 'pcp06', 'pcp24', 'sd']:\n",
    "    bins = []\n",
    "    step = (X[column_name].max() - X[column_name].min()) / 20\n",
    "    for i in range(21):\n",
    "        bins.append(X[column_name].min() + (step * i))\n",
    "    binned = pd.cut(X[column_name], bins=bins).value_counts()\n",
    "    print(binned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8hfKE5kbkGlw"
   },
   "source": [
    "As it was done in a solution using scikit-learn, let's apply *one-hot encoding* for transforming **borough** feature data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "DZAALUMVkKAX",
    "outputId": "55718367-383f-4874-e29a-17c7104d9caa"
   },
   "outputs": [],
   "source": [
    "set(X['borough'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "630r1qeZkO3-"
   },
   "outputs": [],
   "source": [
    "for value in set(X['borough'].values):\n",
    "    new_value = 'borough_{}'.format(value)\n",
    "    X[new_value] = (X['borough'] == value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "cFSrg7X6kRTc",
    "outputId": "538db560-9fe3-4323-95ec-91aa5215aad0"
   },
   "outputs": [],
   "source": [
    "del X['borough']\n",
    "del X['borough_nan']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QPssn8iKkcIZ"
   },
   "source": [
    "Let's prepare the vector of answers: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "lvXBO_vzkf4k",
    "outputId": "76cac5a8-05ef-43ee-d7af-6a872fd3c7ec"
   },
   "outputs": [],
   "source": [
    "Y_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QtapHFCMkig7"
   },
   "outputs": [],
   "source": [
    "merged = pd.merge(X_data, Y_data, on=['id'])\n",
    "Y = merged['pickups'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RZKjburaky2d"
   },
   "source": [
    "Now let's split our training dataset into **N** pieces. We will then train **N** *decision trees* on each subset respectively. This will allow us to create an *ensemble* to improve results and minimize the possibility of overfitting.\n",
    "\n",
    "We'll try to take **N** = 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XkKpnhUeleWC"
   },
   "outputs": [],
   "source": [
    "X_copy = pd.DataFrame(X, copy=True)\n",
    "subsets = []\n",
    "amount = int(len(X_copy) / 10)\n",
    "\n",
    "for x in range(0, 10):\n",
    "    subsets.append(X_copy.sample(n=amount))\n",
    "    X_copy.drop(subsets[x].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7lH7wlusyXdl"
   },
   "source": [
    "Now we have **10** random **subsets** formed from the original set. The next step is to implement the classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eRuFyJ_ylDq"
   },
   "source": [
    "# Implementing Decision Tree (entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4NjnI7La3bjm"
   },
   "source": [
    "*GridSearch* that was used in a solution using scikit-learn showed that *entropy-based* solver gives better results for our problem than *gini-based*. So ***entropy-based*** solver will be used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jO70QxzOGxW4"
   },
   "outputs": [],
   "source": [
    "def entropy(data, attr):\n",
    "#     storing the amount of each value (use bins for continous values here - TODO)\n",
    "    ser = data.groupby(attr).size().to_dict()\n",
    "    \n",
    "    entries = len(data)\n",
    "    entropy = 0.0  # default value\n",
    "    for key in ser.keys():\n",
    "#         counting the probability of the value to occur\n",
    "        probability = float(ser[key])/entries\n",
    "#     counting the entropy of the value by known formula\n",
    "        entropy -= probability * math.log(probability,2)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Zln6cVrGSsWk",
    "outputId": "4372d1ea-9a97-4f40-e439-84ebfe5c6458"
   },
   "outputs": [],
   "source": [
    "entropy(X, 'borough_Bronx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nz7E-b-FWV4q"
   },
   "outputs": [],
   "source": [
    "def split(data, colname, value):\n",
    "#     returns dataframe without given colname which contains only given value of this colname\n",
    "    return data.loc[data[colname] == value, data.columns.drop(colname)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BxhROiKBbhwn"
   },
   "outputs": [],
   "source": [
    "def choose(data):\n",
    "#     helps to choose the best attribute for classification\n",
    "\n",
    "#     TODO improve to use bins!\n",
    "# using infogain term here - may be wrong, for continous data it's recommended to use\n",
    "# gain_ratio, gain_ratio(data, attr) = gain(data, attr) / split_info(data, attr), where\n",
    "# gain (data, attr) = entropy(data)(that means taking target attr as attr argument) - entropy(data, attr) and\n",
    "# split_info(data, attr) = entropy(data, attr). TODO\n",
    "\n",
    "    minimum_entropy = entropy(data, data.columns[0])\n",
    "    best_attr = -1\n",
    "    \n",
    "    values_map = {col: data.groupby(col).size().to_dict() for col in data.columns}\n",
    "    \n",
    "    entropies = []\n",
    "    \n",
    "    for attr in values_map:\n",
    "        new_entropy = 0.0\n",
    "        for value in values_map[attr]:\n",
    "#             data without attr with specific value of this attr only \n",
    "            new_data = split(X_copy, attr, value)\n",
    "            probability = new_data.shape[0]/float(data.shape[0])\n",
    "            new_entropy += probability * entropy(data, attr)\n",
    "        entropies.append(dict(attr=attr, info_amount=new_entropy))\n",
    "    \n",
    "    print(entropies)\n",
    "#     takes attr with minimum entropy as the best one\n",
    "    best_attr = min(entropies, key=operator.itemgetter('info_amount'))\n",
    "    print(f'best attribute is now {best_attr}')\n",
    "    return best_attr['attr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sQkNmGQfcb0y"
   },
   "outputs": [],
   "source": [
    "X_copy = pd.DataFrame(X, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "colab_type": "code",
    "id": "hiq0RBMYcjsQ",
    "outputId": "d5fa5104-23ff-4135-f494-e1f4d2ac3eb9"
   },
   "outputs": [],
   "source": [
    "choose(X_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I7mkPrZ7c_gZ"
   },
   "outputs": [],
   "source": [
    "def majority(classset):\n",
    "#     returns the class that has the most votes\n",
    "# TODO change implementation for regressor! use bins!\n",
    "    count = {}\n",
    "    for attr in classset:\n",
    "        if vote not in count.keys(): count[vote] = 0\n",
    "        count[vote] += 1\n",
    "    sorted_class_count = sorted(count.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sorted_class_count[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S3UR6nmyehv3"
   },
   "outputs": [],
   "source": [
    "def tree(data, labels):\n",
    "#     if only one entry - return it\n",
    "    if data.shape[0] == 1:\n",
    "        return data.loc[:0]\n",
    "#     if only one column left - return the majority of it's values (TODO improve function)\n",
    "    if data.columns.size == 1:\n",
    "        return majority(data)\n",
    "#     else choose best feat, create a node, go recursively.\n",
    "    best_feat = choose(data)\n",
    "    print(labels)\n",
    "    the_tree = {best_feat:{}}\n",
    "    labels.remove(best_feat)\n",
    "    print(best_feat)\n",
    "    feat_values = data[best_feat]\n",
    "    unique_vals = feat_values.unique()\n",
    "    for value in unique_vals:\n",
    "        print(f'\\n {value} ({len(labels)} feats remaining)')\n",
    "        sublabels = labels.copy()\n",
    "        the_tree[best_feat][value] = tree(split(data, best_feat, value), sublabels)\n",
    "    return the_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q5z-kZ-1fbQg"
   },
   "outputs": [],
   "source": [
    "X_copy = pd.DataFrame(X, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196374
    },
    "colab_type": "code",
    "id": "9G9jj5wUfgwp",
    "outputId": "2197f0cd-8f4f-4bb2-ee0e-7cc00ed7c346"
   },
   "outputs": [],
   "source": [
    "tree(X_copy, set(X_copy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XN5uJRsjC48x"
   },
   "outputs": [],
   "source": [
    "entropy_map = {}\n",
    "for feat in set(X_copy):\n",
    "  entropy_map[feat] = entropy(X_copy, feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "txLs4nXrEfag"
   },
   "outputs": [],
   "source": [
    "entropy_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wxHX6HmoEgvf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Uber_pickups_random_forest_entropy.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
